{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+https://github.com/huggingface/diffusers.git transformers accelerate\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from diffusers import StableUnCLIPImg2ImgPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the StableUnCLIP Image variations pipeline\n",
    "pipe = StableUnCLIPImg2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1-unclip\", torch_dtype=torch.float16, variation=\"fp16\"\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the train_images NumPy array\n",
    "train_images = np.load(\"/mnt/sphere/projects/simon/Visual-Reconstruction/data/thingseeg2_metadata/train_images.npy\", mmap_mode='r')\n",
    "# Convert each image to a PIL image\n",
    "pil_images = [Image.fromarray(image).convert(\"RGB\") for image in train_images]\n",
    "init_image = pil_images[1]\n",
    "# Example: Display the first image\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the init_image\n",
    "plt.imshow(init_image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image = Image.open('/mnt/sphere/projects/simon/Visual-Reconstruction/data/thingseeg2_metadata/test_images_direct/0.png').convert(\"RGB\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the init_image\n",
    "plt.imshow(init_image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = pipe._execution_device\n",
    "noise_level = torch.tensor([0], device=device)\n",
    "embedding = pipe._encode_image(init_image, device=device, batch_size=1, num_images_per_prompt=1, do_classifier_free_guidance=True,noise_level=noise_level,generator=None, image_embeds = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding[1].unsqueeze(0)\n",
    "negative_prompt_embeds = torch.zeros_like(embedding)\n",
    "embedding = torch.cat([negative_prompt_embeds, embedding])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(embedding[1].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding[1][1025:1025+512].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "images = pipe.decode(embedding).images\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the init_image\n",
    "plt.imshow(images[0])\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get image from URL\n",
    "url = \"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/stable_unclip/tarsila_do_amaral.png\"\n",
    "response = requests.get(url)\n",
    "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot the init_image\n",
    "plt.imshow(init_image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pipe to make the variation\n",
    "torch.manual_seed(0)\n",
    "images = pipe(init_image).images\n",
    "# images[0].save(\"tarsila_variation.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the init_image\n",
    "plt.imshow(images[0])\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = pipe._execution_device\n",
    "noise_level = torch.tensor([0], device=device)\n",
    "embedding = pipe._encode_image(init_image, device=device, batch_size=1, num_images_per_prompt=1, do_classifier_free_guidance=True,noise_level=noise_level,generator=None, image_embeds = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve the corresponding axis\n",
    "embedding = embedding[1].unsqueeze(0)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra_portion = embedding[0,1024:]\n",
    "extra_portion = torch.cat([torch.ones(512), torch.zeros(512)])\n",
    "\n",
    "embedding = embedding[0,:1024]\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(extra_portion.detach().cpu().numpy())\n",
    "extra_portion.detach().cpu().numpy().sum(), extra_portion[511]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(\"/mnt/sphere/projects/simon/Visual-Reconstruction/cache/thingseeg2_preproc/predicted_embeddings/sub-01/regress_clipvision1024.npy\", mmap_mode='r')\n",
    "embedding = torch.tensor(embeddings[32], device=device, dtype=torch.float16)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_ones = torch.ones(512, dtype=torch.float16, device=device)\n",
    "torch_zeros = torch.zeros(512, dtype=torch.float16, device=device)\n",
    "extra_portion = torch.cat([torch_ones, torch_zeros])\n",
    "embedding = torch.cat([embedding, extra_portion]).unsqueeze(0)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt_embeds = torch.zeros_like(embedding)\n",
    "embedding = torch.cat([negative_prompt_embeds, embedding])\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "images = pipe.decode(embedding).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the init_image\n",
    "plt.imshow(images[0])\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
